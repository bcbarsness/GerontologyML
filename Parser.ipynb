{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import local_parameters\n",
    "import urllib.request \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie = JSESSIONID=''\n",
    "headers = {\n",
    "    'User-Agent': local_parameters.UserAgent,\n",
    "    'Cookie' : local_parameters.Cookie}\n",
    "session = requests.session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test=False\n",
    "issueListFilename = \"2001-2008URLs.csv\"\n",
    "_source_html_path = 'source_html2'\n",
    "get_year = 2003\n",
    "count = 78\n",
    "first = 1\n",
    "\n",
    "def save_html(year,source_html): \n",
    "    global count, first, my_year\n",
    "    if first:\n",
    "        first = 0\n",
    "        my_year = year\n",
    "        count += 1\n",
    "    else:\n",
    "        if my_year == year:\n",
    "            count += 1\n",
    "        else: \n",
    "            my_year = year\n",
    "            count = 1\n",
    "            \n",
    "    print(str(year) + '-' + str(count) + '.html')\n",
    "    f = open(os.path.join(_source_html_path, str(year) + '-' + str(count) + '.html'), 'w')\n",
    "    f.write(source_html)\n",
    "    f.close\n",
    "\n",
    "def get_url_source_html(url):\n",
    "    response = session.get(str(url), headers=headers)\n",
    "    redirected_url = response.history[-1].url\n",
    "    print(redirected_url)\n",
    "    response = session.get(redirected_url, headers=headers)\n",
    "    return response.text\n",
    "\n",
    "def process_url(row):\n",
    "    print(str(count) + \": \" + row.url)\n",
    "    source_html = get_url_source_html(row.url)\n",
    "    #time.sleep(5)\n",
    "    save_html(row.year, source_html)\n",
    "    #time.sleep(10)\n",
    "\n",
    "         \n",
    "if test == False:\n",
    "    df = pd.read_csv(issueListFilename)\n",
    "    #df = df[df.year == get_year] \n",
    "    for index, row in df.iterrows():\n",
    "       process_url(row)\n",
    "    \n",
    "else:\n",
    "    #url = 'https://doi.org/10.1093/geront/gnx189'\n",
    "    year = '0000'\n",
    "    url = 'https://doi.org/10.1093/geront/43.2.192'\n",
    "    data = [[year,url]]\n",
    "    df = pd.DataFrame(data,columns=['year','url'])\n",
    "    \n",
    "    process_url(df.url.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import urllib.request\n",
    "#d = urllib.request.urlopen(\"http://www.google.co.uk\")\n",
    "#d.getheader('Set-Cookie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_source_html(url):\n",
    "    response = session.get(url, headers=headers)\n",
    "    redirected_url = response.history[-1].url\n",
    "    print(redirected_url)\n",
    "\n",
    "    response = session.get(redirected_url, headers=headers)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(myarticle):\n",
    "    return BeautifulSoup(myarticle, \"lxml\")\n",
    "\n",
    "def strip_hyperlinks(mysoup):\n",
    "    for a in mysoup.findAll('a'):\n",
    "        a.replaceWith(\" %s \" % a.string)\n",
    "    return(mysoup)\n",
    "\n",
    "def strip_references(mysoup):\n",
    "    for div in mysoup.find_all(\"div\", {'class':'ref-list'}): \n",
    "        div.decompose()\n",
    "    return(mysoup)\n",
    "\n",
    "def strip_copywrite(mysoup):\n",
    "    for div in mysoup.find_all(\"div\", {'class':'copyright copyright-statement'}): \n",
    "        div.decompose()\n",
    "    return(mysoup)\n",
    "\n",
    "def strip_tables(mysoup):\n",
    "    codetags = mysoup.find_all('table')\n",
    "    for codetag in codetags:\n",
    "       codetag.extract()\n",
    "    return(mysoup)\n",
    "\n",
    "def strip_rnt(mysoup):\n",
    "    return(mysoup.get_text().replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' '))\n",
    " \n",
    "def clean_article(mysoup):\n",
    "    #soup = get_soup(myarticle)\n",
    "    mysoup = strip_hyperlinks(mysoup)\n",
    "    mysoup = strip_references(mysoup)\n",
    "    mysoup = strip_copywrite(mysoup)\n",
    "    mysoup = strip_tables(mysoup)\n",
    "    soup_text = strip_rnt(mysoup)\n",
    "    return soup_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_source_html(source_html):\n",
    "    soup = BeautifulSoup(source_html, \"html.parser\")\n",
    "\n",
    "    raw_html.append(soup.get_text())\n",
    "    \n",
    "    citation_date = soup.find(\"div\", {\"class\": \"citation-date\"}).text\n",
    "    citation_dates.append(citation_date)\n",
    "\n",
    "    citation = soup.find(attrs={'class': 'ww-citation-primary'}).text\n",
    "    citations.append(citation)\n",
    "\n",
    "    article_body = soup.find(\"div\", {\"class\": \"article-body\"})\n",
    "    articles.append(article_body)\n",
    "    \n",
    "    types.append(soup.find(attrs={'property': 'og:type'}).attrs['content'])\n",
    "    titles.append(soup.find(attrs={'property': 'og:title'}).attrs['content'])\n",
    "    descriptions.append(soup.find(attrs={'property': 'og:description'}).attrs['content'])\n",
    "    urls.append(soup.find(attrs={'property': 'og:url'}).attrs['content'])\n",
    "    updated_times.append(soup.find(attrs={'property': 'og:updated_time'}).attrs['content'])\n",
    "    site_names.append(soup.find(attrs={'property': 'og:site_name'}).attrs['content'])\n",
    "    \n",
    "    article = clean_article(article_body)\n",
    "    clean_articles.append(article)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test=False\n",
    "citations = []\n",
    "citation_dates = []\n",
    "articles = []\n",
    "clean_articles = []\n",
    "types = []\n",
    "titles = []\n",
    "descriptions = []\n",
    "urls = []\n",
    "updated_times = []\n",
    "site_names = []\n",
    "raw_html = []\n",
    "\n",
    "issueListFilename = \"2001-2008URLs.csv\"\n",
    "get_year = 2003\n",
    "count = 0\n",
    "\n",
    "def process_url(my_url):\n",
    "    global count\n",
    "    print(str(count) + \": \" + my_url)\n",
    "    source_html = get_url_source_html(my_url)\n",
    "    #time.sleep(5)\n",
    "    #print(source_html)\n",
    "    parse_source_html(source_html)\n",
    "    #time.sleep(10)\n",
    "    count += 1\n",
    "         \n",
    "if test == False:\n",
    "    df = pd.read_csv(issueListFilename)\n",
    "    df = df[df.year == get_year] \n",
    "    print(len(df.index))\n",
    "    df.url.apply(lambda x: process_url(x))\n",
    "    \n",
    "else:\n",
    "    #url = 'https://doi.org/10.1093/geront/gnx189'\n",
    "    url = 'https://doi.org/10.1093/geront/43.2.192'\n",
    "    print(url)\n",
    "    source_html = get_url_source_html(url)\n",
    "    parse_source_html(source_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'citation_date': citation_dates,\n",
    "                    'citation': citations,\n",
    "                    'title': titles,\n",
    "                    'type': types,\n",
    "                    'description': descriptions,\n",
    "                    'url': urls,\n",
    "                    'updated_time': updated_times,\n",
    "                    'site_name': site_names,\n",
    "                    'article': articles,\n",
    "                    'clean_article': clean_articles,\n",
    "                    'raw_html' : raw_html\n",
    "                    },columns = ['citation_date', 'citation', 'title', 'type', 'description', 'url', 'updated_time', 'site_name', 'article', 'clean_article', 'raw_html'])\n",
    "print(df.info())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_article\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "def my_print(my_x):\n",
    "    global count\n",
    "    print(count, len(my_x))\n",
    "    count += 1\n",
    "\n",
    "df[\"clean_article\"].apply(lambda x: my_print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['clean_article'].loc[129]\n",
    "df.loc[129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "remove_indices = []\n",
    "def get_short_articles(my_x):\n",
    "    global remove_indices, count\n",
    "    if len(my_x) < 500:\n",
    "        remove_indices.append(count)\n",
    "    count += 1\n",
    "\n",
    "df[\"clean_article\"].apply(lambda x: get_short_articles(x))\n",
    "print(\"Initial set: \", len(df))\n",
    "print(\"Remove set: \" + str(len(remove_indices)))\n",
    "print(remove_indices)\n",
    "\n",
    "df_reduced = df.drop(remove_indices)\n",
    "print(\"After removal: \" + str(len(df_reduced)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "def my_print(my_x):\n",
    "    global count\n",
    "    print(count, len(my_x))\n",
    "    count += 1\n",
    "\n",
    "df_reduced[\"clean_article\"].apply(lambda x: my_print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_filename = 'articles' + str(get_year) + '.txt'\n",
    "with open(os.path.join(base_filename),'w') as outfile:\n",
    "    df_reduced[\"clean_article\"].apply(lambda x: outfile.write(x + '\\n'))\n",
    "outfile.close()\n",
    "num_lines = sum(1 for line in open(base_filename))\n",
    "print(num_lines)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "pickle_file = 'data_' + str(get_year) + '.pkl.gzip'\n",
    "df_reduced.to_pickle(pickle_file, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filename = 'articles' + str(get_year) + '.csv'\n",
    "df_reduced.to_csv(result_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
