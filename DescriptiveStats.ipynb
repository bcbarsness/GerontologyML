{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bcbarsness/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bcbarsness/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#text = open('FullIssueSourceCleanTXT.txt').read()\n",
    "text = open('articles.txt').read()\n",
    "#\n",
    "#allWords = tokenizer.tokenize(text)\n",
    "allWords = nltk.tokenize.word_tokenize(text)\n",
    "allWordDist = nltk.FreqDist(w.lower() for w in allWords)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "allWordExceptStopDist = nltk.FreqDist(w.lower() for w in allWords if w not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mostCommon= allWordDist.most_common(10).keys()\n",
    "mostCommon= allWordExceptStopDist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 25995), ('.', 16557), (')', 7769), ('(', 7760), (';', 3156), ('care', 2662), ('the', 2311), ('%', 1880), ('=', 1779), (':', 1547)]\n"
     ]
    }
   ],
   "source": [
    "print(mostCommon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 25995), ('the', 21268)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "#raw = open('FullIssueSourceCleanTXT.txt').read()\n",
    "raw = open('articles.txt').read()\n",
    "\n",
    "#raw = open(path, 'rU').read()\n",
    "tokens = word_tokenize(raw)\n",
    "#words = corpus.words(raw)\n",
    "fdist = FreqDist(tokens)\n",
    "fdist.most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('care', 2662),\n",
       " ('study', 1285),\n",
       " ('may', 1246),\n",
       " ('older', 1206),\n",
       " ('data', 1147),\n",
       " ('caregivers', 1133),\n",
       " ('health', 1114),\n",
       " ('nursing', 1104),\n",
       " ('table', 1041),\n",
       " ('home', 1017)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "#filename = 'FullIssueSourceCleanTXT.txt'\n",
    "filename = 'articles.txt'\n",
    "\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "\n",
    "#print(words[:100])\n",
    "fdist = FreqDist(words)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('care', 2764),\n",
       " ('caregiv', 2174),\n",
       " ('studi', 1802),\n",
       " ('use', 1709),\n",
       " ('age', 1511),\n",
       " ('resid', 1491),\n",
       " ('home', 1424),\n",
       " ('group', 1419),\n",
       " ('nurs', 1279),\n",
       " ('may', 1246)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "#filename = 'FullIssueSourceCleanTXT.txt'\n",
    "filename = 'articles.txt'\n",
    "\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(len(tokens))\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "# stemming of words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in words]\n",
    "#print(words[:100])\n",
    "fdist = FreqDist(stemmed)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words 261145\n",
      "[('care', 2662), ('study', 1285), ('may', 1246), ('older', 1206), ('data', 1147), ('caregivers', 1133), ('health', 1114), ('nursing', 1104), ('table', 1041), ('home', 1017), ('one', 954), ('residents', 930), ('research', 911), ('also', 847), ('age', 836), ('living', 816), ('p', 812), ('group', 802), ('large', 743), ('used', 723)]\n",
      "Stemmed 261145\n",
      "[('care', 2764), ('caregiv', 2174), ('studi', 1802), ('use', 1709), ('age', 1511), ('resid', 1491), ('home', 1424), ('group', 1419), ('nurs', 1279), ('may', 1246), ('older', 1206), ('research', 1170), ('data', 1147), ('differ', 1120), ('health', 1114), ('tabl', 1067), ('live', 1050), ('report', 1017), ('provid', 1016), ('measur', 1016)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Words\", len(words))\n",
    "print(FreqDist(words).most_common(20))\n",
    "print(\"Stemmed\", len(stemmed))\n",
    "print(FreqDist(stemmed).most_common(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
